{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "## Version futuresales lib required = 0.1.8\n",
    "\n",
    "# !pip install -i https://test.pypi.org/simple/ futuresales_denissimo==0.1.8"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "from tqdm._tqdm_notebook import tqdm_notebook\n",
    "from tqdm._tqdm_notebook import tqdm\n",
    "tqdm_notebook.pandas()\n",
    "\n",
    "import pickle\n",
    "\n",
    "import futuresales as fs"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/tmp/ipykernel_13899/2966614061.py:4: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.*` instead of `tqdm._tqdm_notebook.*`\n",
      "  from tqdm._tqdm_notebook import tqdm_notebook\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "from futuresales.utils import load_credentials\n",
    "import neptune.new as neptune\n",
    "\n",
    "cred = load_credentials('../credentials.json')['projects']['fs-feature-space']\n",
    "\n",
    "run = neptune.init(\n",
    "    api_token=cred['api_token'],\n",
    "    project=cred['project'],\n",
    "    tags=['extended', 'baseline']\n",
    ")  # your credentials\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Info (NVML): Driver Not Loaded. GPU usage metrics may not be reported. For more information, see https://docs-legacy.neptune.ai/logging-and-managing-experiment-results/logging-experiment-data.html#hardware-consumption \n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "https://app.neptune.ai/denissimo/fs-feature-space/e/FSFEAT-4\n",
      "Remember to stop your run once youâ€™ve finished logging your metadata (https://docs.neptune.ai/api-reference/run#stop). It will be stopped automatically only when the notebook kernel/interactive console is terminated.\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "CONFIG = {\n",
    "    'FETCH_DATA': True\n",
    "}"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "run['dataset_dependecy'] = {\n",
    "    'original': 'kaggle competitions download -c competitive-data-science-predict-future-sales',\n",
    "}"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "df_provider = fs.distribution.DatasetProvider()\n",
    "\n",
    "df_provider.file_list = [\n",
    "    '/home/denissimo/Repo/fs_project/datasets/sample_submission.csv',\n",
    "    '/home/denissimo/Repo/fs_project/datasets/test.csv',\n",
    "    '/home/denissimo/Repo/fs_project/datasets/shops.csv',\n",
    "    '/home/denissimo/Repo/fs_project/datasets/item_categories.csv',\n",
    "    '/home/denissimo/Repo/fs_project/datasets/sales_train.csv',\n",
    "    '/home/denissimo/Repo/fs_project/datasets/items.csv'\n",
    "]\n",
    "\n",
    "datasets = df_provider.get_dataset()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "sales = datasets['sales_train.csv']\n",
    "items = datasets['items.csv']\n",
    "categories = datasets['item_categories.csv']\n",
    "shops = datasets['shops.csv']\n",
    "\n",
    "sales.date = sales.date.astype('datetime64[ns]')\n",
    "\n",
    "\n",
    "from datetime import date\n",
    "sales = sales.loc[sales.date < np.datetime64(date(2015, 11, 1))]\n",
    "\n",
    "print(\"Before:\", sales.shape)\n",
    "\n",
    "sales_train = sales[\n",
    "    (sales[\"item_cnt_day\"] < 550)\n",
    "    & (sales[\"item_price\"] > 0)\n",
    "    & (sales[\"item_price\"] < 60000)\n",
    "].copy()\n",
    "print(\"After:\", sales_train.shape)\n",
    "\n",
    "sales.head()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Before: (2896782, 6)\n",
      "After: (2896775, 6)\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>date_block_num</th>\n",
       "      <th>shop_id</th>\n",
       "      <th>item_id</th>\n",
       "      <th>item_price</th>\n",
       "      <th>item_cnt_day</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2013-02-01</td>\n",
       "      <td>0</td>\n",
       "      <td>59</td>\n",
       "      <td>22154</td>\n",
       "      <td>999.00</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2013-03-01</td>\n",
       "      <td>0</td>\n",
       "      <td>25</td>\n",
       "      <td>2552</td>\n",
       "      <td>899.00</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2013-05-01</td>\n",
       "      <td>0</td>\n",
       "      <td>25</td>\n",
       "      <td>2552</td>\n",
       "      <td>899.00</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2013-06-01</td>\n",
       "      <td>0</td>\n",
       "      <td>25</td>\n",
       "      <td>2554</td>\n",
       "      <td>1709.05</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2013-01-15</td>\n",
       "      <td>0</td>\n",
       "      <td>25</td>\n",
       "      <td>2555</td>\n",
       "      <td>1099.00</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        date  date_block_num  shop_id  item_id  item_price  item_cnt_day\n",
       "0 2013-02-01               0       59    22154      999.00           1.0\n",
       "1 2013-03-01               0       25     2552      899.00           1.0\n",
       "2 2013-05-01               0       25     2552      899.00          -1.0\n",
       "3 2013-06-01               0       25     2554     1709.05           1.0\n",
       "4 2013-01-15               0       25     2555     1099.00           1.0"
      ]
     },
     "metadata": {},
     "execution_count": 12
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "from extraction_configs.pipeline_configs import make_indexes, make_task_df\n",
    "from extraction_configs.generators_config import make_baseline_train_generator\n",
    "from extraction_configs.aggregation_configs import make_baseline_train_aggregation, make_baseline_submission_aggregation\n",
    "from futuresales.distribution import to_pickle, from_pickle\n",
    "from futuresales.features import FeatureExtractor\n",
    "\n",
    "pipelines = {\n",
    "    'idx': make_indexes(shops, categories),\n",
    "}\n",
    "\n",
    "task_df = {}\n",
    "task_df['idx'] = pipelines['idx']['id_merging_stage'](items)\n",
    "\n",
    "task_df['idx'].to_csv('../tmp/idx.csv')\n",
    "\n",
    "pipelines['task_df'] = make_task_df(items, shops, categories, task_df['idx'])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "from futuresales.pipeline import Pipeline\n",
    "\n",
    "pipeline = Pipeline(\n",
    "    tasks=pipelines['task_df'], \n",
    "    task_queue = [\n",
    "        'id_merging_stage',\n",
    "        'summarizing_and_name_merging_stage',\n",
    "        'date_block_num_renaming',\n",
    "        'object_id_encoding',\n",
    "    ]\n",
    ")\n",
    "\n",
    "pipeline_test = pipeline(sales)\n",
    "pipeline_train = pipeline(sales_train)\n",
    "\n",
    "if not CONFIG['FETCH_DATA']:\n",
    "    _ = pipeline_test.proceed_all()\n",
    "    _ = pipeline_train.proceed_all()\n",
    "    task_df['test'] = pipeline_test.result_storage['object_id_encoding']\n",
    "    task_df['train'] = pipeline_train.result_storage['object_id_encoding']\n",
    "    task_df['test'].to_csv('../tmp/task_df_test.csv')\n",
    "    task_df['train'].to_csv('../tmp/task_df_train.csv')\n",
    "else:\n",
    "    task_df['test'] = pd.read_csv('../tmp/task_df_test.csv')\n",
    "    task_df['train'] = pd.read_csv('../tmp/task_df_train.csv')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "source": [
    "def dyn_agg(months):\n",
    "    return {f'dynamic_aggregation_{k}_win_{months}':f'dynamic_aggregation_{k}_all' for k in ['mean', 'std', 'min', 'max']}\n",
    "\n",
    "baseline = {}\n",
    "\n",
    "file_map = {\n",
    "    30: 'train_set',\n",
    "    31: 'test_set',\n",
    "    32: 'validation_set',\n",
    "}\n",
    "\n",
    "train_set = None\n",
    "\n",
    "for train_max in range(19, 31):\n",
    "    print(f'\\nProcessing train set month - {train_max}')\n",
    "    baseline_extractor = FeatureExtractor(\n",
    "        make_baseline_train_generator(train_max), \n",
    "        make_baseline_train_aggregation(train_max), \n",
    "        ['id'])\n",
    "    baseline[train_max] = baseline_extractor({\n",
    "        'original': task_df['train'],\n",
    "        'test': task_df['test']\n",
    "        })\n",
    "    features = baseline[train_max]['features']\n",
    "    for key in features:\n",
    "        if not key in ['valid_target', 'target', 'lags']:\n",
    "            features[key].columns = [\n",
    "                f'{key}_{agg_type}' for agg_type in features[key].columns.values\n",
    "                ]\n",
    "    features = pd.concat(\n",
    "        features.values(), axis=1, join='inner')\n",
    "    features = features.rename(dyn_agg(train_max + 1), axis=1)\n",
    "    features['month'] = train_max % 12\n",
    "    features['num_month'] = train_max\n",
    "    if train_set is None:\n",
    "        train_set = features\n",
    "    else:\n",
    "        train_set = pd.concat([train_set, features], axis=0)\n",
    "    from gc import collect\n",
    "    collect()\n",
    "\n",
    "train_set.to_csv(f'../tmp/ext_baseline_{file_map[30]}.csv')"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "Processing train set month - 19\n",
      "Subseries stage: id_sales\n",
      "Subseries stage: id_sales_test\n",
      "Subseries stage: train_series\n",
      "Subseries stage: diff_1\n",
      "Subseries stage: diff_2\n",
      "Subseries stage: lags\n",
      "Subseries stage: lags_12\n",
      "\n",
      "Processing train set month - 20\n",
      "Subseries stage: id_sales\n",
      "Subseries stage: id_sales_test\n",
      "Subseries stage: train_series\n",
      "Subseries stage: diff_1\n",
      "Subseries stage: diff_2\n",
      "Subseries stage: lags\n",
      "Subseries stage: lags_12\n",
      "\n",
      "Processing train set month - 21\n",
      "Subseries stage: id_sales\n",
      "Subseries stage: id_sales_test\n",
      "Subseries stage: train_series\n",
      "Subseries stage: diff_1\n",
      "Subseries stage: diff_2\n",
      "Subseries stage: lags\n",
      "Subseries stage: lags_12\n",
      "\n",
      "Processing train set month - 22\n",
      "Subseries stage: id_sales\n",
      "Subseries stage: id_sales_test\n",
      "Subseries stage: train_series\n",
      "Subseries stage: diff_1\n",
      "Subseries stage: diff_2\n",
      "Subseries stage: lags\n",
      "Subseries stage: lags_12\n",
      "\n",
      "Processing train set month - 23\n",
      "Subseries stage: id_sales\n",
      "Subseries stage: id_sales_test\n",
      "Subseries stage: train_series\n",
      "Subseries stage: diff_1\n",
      "Subseries stage: diff_2\n",
      "Subseries stage: lags\n",
      "Subseries stage: lags_12\n",
      "\n",
      "Processing train set month - 24\n",
      "Subseries stage: id_sales\n",
      "Subseries stage: id_sales_test\n",
      "Subseries stage: train_series\n",
      "Subseries stage: diff_1\n",
      "Subseries stage: diff_2\n",
      "Subseries stage: lags\n",
      "Subseries stage: lags_12\n",
      "\n",
      "Processing train set month - 25\n",
      "Subseries stage: id_sales\n",
      "Subseries stage: id_sales_test\n",
      "Subseries stage: train_series\n",
      "Subseries stage: diff_1\n",
      "Subseries stage: diff_2\n",
      "Subseries stage: lags\n",
      "Subseries stage: lags_12\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "baseline"
   ],
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'baseline' is not defined",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_14063/3547077931.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mbaseline\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'baseline' is not defined"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "for train_max in range(31, 33):\n",
    "    print(f'\\nProcessing set - {file_map[train_max]}')\n",
    "    baseline_extractor = FeatureExtractor(\n",
    "        make_baseline_train_generator(train_max), \n",
    "        make_baseline_train_aggregation(train_max), \n",
    "        ['id'])\n",
    "    baseline[file_map[train_max]] = baseline_extractor({\n",
    "        'original': task_df['train'],\n",
    "        'test': task_df['test']\n",
    "        })\n",
    "    features = baseline[file_map[train_max]]['features']\n",
    "    for key in features:\n",
    "        if not key in ['valid_target', 'target', 'lags']:\n",
    "            features[key].columns = [\n",
    "                f'{key}_{agg_type}' for agg_type in features[key].columns.values\n",
    "                ]\n",
    "    features = pd.concat(\n",
    "        features.values(), axis=1, join='inner')\n",
    "    features = features.rename(dyn_agg(train_max + 1), axis=1)\n",
    "    features['month'] = train_max % 12\n",
    "    features['num_month'] = train_max\n",
    "    features.to_csv(f'../tmp/ext_baseline_{file_map[train_max]}.csv')"
   ],
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'file_map' is not defined",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_13720/2215003570.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtrain_max\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m31\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m33\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'\\nProcessing set - {file_map[train_max]}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     baseline_extractor = FeatureExtractor(\n\u001b[1;32m      4\u001b[0m         \u001b[0mmake_baseline_train_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_max\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mmake_baseline_train_aggregation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_max\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'file_map' is not defined"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "feature_info = {\n",
    "    key: baseline[file_map[30]]['features'][key].columns.to_list() for key in baseline[file_map[30]]['features'].keys()\n",
    "}"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "run['features'] = feature_info\n",
    "run['extraction_configs'].upload_files([\n",
    "    '../featuring/extraction_configs/aggregation_configs.py',\n",
    "    '../featuring/extraction_configs/pipeline_configs.py',\n",
    "    '../featuring/extraction_configs/generators_config.py',\n",
    "    ])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "run['extraction_notebook'].upload_files('../featuring/baseline_extraction.ipynb')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "run.stop()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Shutting down background jobs, please wait a moment...\n",
      "Done!\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Waiting for the remaining 1 operations to synchronize with Neptune. Do not kill this process.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "All 1 operations synced, thanks for waiting!\n"
     ]
    }
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.10",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.10 64-bit ('env': venv)"
  },
  "interpreter": {
   "hash": "d6f0a83fd70573169f91d6afa8cd6daf12135fd479a43538d29ef02678dc9121"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}