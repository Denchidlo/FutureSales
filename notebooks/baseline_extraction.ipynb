{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "source": [
    "from lib import *\n",
    "\n",
    "Pipeline = pipeline.Pipeline\n",
    "RegressionValidator = pipeline.RegressionValidator\n",
    "\n",
    "FeatureExtractor = extractor.FeatureExtractor\n",
    "\n",
    "DatasetProvider = provider.DatasetProvider\n",
    "DatasetUploader = provider.DatasetUploader\n",
    "ExpandedWindowIterator = subset_extraction.ExpandedWindowIterator\n",
    "EntityIterator = subset_extraction.EntityIterator"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "from tqdm._tqdm_notebook import tqdm_notebook\n",
    "from tqdm._tqdm_notebook import tqdm\n",
    "tqdm_notebook.pandas()\n",
    "\n",
    "import pickle\n",
    "\n",
    "\n",
    "pd.plotting.register_matplotlib_converters()\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "tr = transformers\n",
    "agg = aggregators"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "source": [
    "CONFIG = {\n",
    "    'FETCH_DATA': True,\n",
    "}"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "source": [
    "df_provider = DatasetProvider()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "source": [
    "dataset_paths = [\n",
    "    '/home/denissimo/Repo/fs_project/datasets/sample_submission.csv',\n",
    "    '/home/denissimo/Repo/fs_project/datasets/test.csv',\n",
    "    '/home/denissimo/Repo/fs_project/datasets/shops.csv',\n",
    "    '/home/denissimo/Repo/fs_project/datasets/item_categories.csv',\n",
    "    '/home/denissimo/Repo/fs_project/datasets/sales_train.csv',\n",
    "    '/home/denissimo/Repo/fs_project/datasets/items.csv'\n",
    "]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "source": [
    "df_provider.file_list = dataset_paths\n",
    "\n",
    "datasets = df_provider.get_dataset()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "source": [
    "sales = datasets['sales_train.csv']\n",
    "items = datasets['items.csv']\n",
    "categories = datasets['item_categories.csv']\n",
    "shops = datasets['shops.csv']"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "source": [
    "sales.date = sales.date.astype('datetime64[ns]')\n",
    "\n",
    "print(\"Before:\", sales.shape)\n",
    "\n",
    "from datetime import date\n",
    "\n",
    "sales = sales.loc[sales.date < np.datetime64(date(2015, 11, 1))]\n",
    "\n",
    "sales_train = sales[\n",
    "    (sales[\"item_cnt_day\"] < 1000)\n",
    "    & (sales[\"item_price\"] > 0)\n",
    "    & (sales[\"item_price\"] < 60000)\n",
    "].copy()\n",
    "print(\"After:\", sales_train.shape)\n",
    "\n",
    "sales.head()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Before: (2935849, 6)\n",
      "After: (2896778, 6)\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>date_block_num</th>\n",
       "      <th>shop_id</th>\n",
       "      <th>item_id</th>\n",
       "      <th>item_price</th>\n",
       "      <th>item_cnt_day</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2013-02-01</td>\n",
       "      <td>0</td>\n",
       "      <td>59</td>\n",
       "      <td>22154</td>\n",
       "      <td>999.00</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2013-03-01</td>\n",
       "      <td>0</td>\n",
       "      <td>25</td>\n",
       "      <td>2552</td>\n",
       "      <td>899.00</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2013-05-01</td>\n",
       "      <td>0</td>\n",
       "      <td>25</td>\n",
       "      <td>2552</td>\n",
       "      <td>899.00</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2013-06-01</td>\n",
       "      <td>0</td>\n",
       "      <td>25</td>\n",
       "      <td>2554</td>\n",
       "      <td>1709.05</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2013-01-15</td>\n",
       "      <td>0</td>\n",
       "      <td>25</td>\n",
       "      <td>2555</td>\n",
       "      <td>1099.00</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        date  date_block_num  shop_id  item_id  item_price  item_cnt_day\n",
       "0 2013-02-01               0       59    22154      999.00           1.0\n",
       "1 2013-03-01               0       25     2552      899.00           1.0\n",
       "2 2013-05-01               0       25     2552      899.00          -1.0\n",
       "3 2013-06-01               0       25     2554     1709.05           1.0\n",
       "4 2013-01-15               0       25     2555     1099.00           1.0"
      ]
     },
     "metadata": {},
     "execution_count": 29
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "source": [
    "import pickle\n",
    "\n",
    "def load_pickle(dataset, name):\n",
    "    dataset.to_pickle(name + '.pkl')\n",
    "\n",
    "def from_pickle(path):\n",
    "    with open(path, 'rb') as reader:\n",
    "        return pickle.load(reader)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "source": [
    "index_preprocessing = {}\n",
    "\n",
    "index_preprocessing['id_merging_stage'] = lambda dataset: dataset.merge(\n",
    "    shops,\n",
    "    how='cross'\n",
    ").merge(\n",
    "    categories,\n",
    "    on='item_category_id'\n",
    ").reset_index().rename({'index': 'id'}, axis=1)\n",
    "\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "source": [
    "task_df = {}\n",
    "task_df['idx'] = index_preprocessing['id_merging_stage'](items)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "source": [
    "data_preprocessing = {}\n",
    "\n",
    "# Add corresponding category and shop id's to each sale\n",
    "data_preprocessing['id_merging_stage'] = lambda dataset: dataset.merge(\n",
    "    items, \n",
    "    on='item_id'\n",
    ").merge(\n",
    "    shops,\n",
    "    on='shop_id'\n",
    ").merge(\n",
    "    categories,\n",
    "    on='item_category_id'\n",
    ")\n",
    "\n",
    "# Add summary among shop_id and category_id above similar time periods (daily intervals)\n",
    "data_preprocessing['summarizing_and_name_merging_stage'] = lambda dataset: dataset.groupby(\n",
    "    ['date', 'date_block_num', 'shop_id', 'item_category_id', 'item_id']\n",
    ").agg({'item_cnt_day': np.sum, 'item_price': np.mean}).reset_index().sort_values('date')\n",
    "\n",
    "data_preprocessing['date_block_num_renaming'] = lambda dataset: dataset.rename(columns={'date_block_num': 'month_block'}, inplace=True)\n",
    "\n",
    "data_preprocessing['object_id_encoding'] = lambda dataset: dataset.merge(\n",
    "    task_df['idx'][['id', 'shop_id', 'item_id']], \n",
    "    on=['shop_id', 'item_id'])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "source": [
    "pipeline = Pipeline(\n",
    "    tasks=data_preprocessing, \n",
    "    task_queue = [\n",
    "        'id_merging_stage',\n",
    "        'summarizing_and_name_merging_stage',\n",
    "        'date_block_num_renaming',\n",
    "        'object_id_encoding',\n",
    "    ]\n",
    ")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "source": [
    "pipeline_test = pipeline(sales)\n",
    "pipeline_train = pipeline(sales_train)\n",
    "\n",
    "if not CONFIG['FETCH_DATA']:\n",
    "    _ = pipeline_test.proceed_all()\n",
    "    _ = pipeline_train.proceed_all()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "source": [
    "if CONFIG['FETCH_DATA'] == False:\n",
    "    task_df['test'] = pipeline_test.result_storage['object_id_encoding']\n",
    "    task_df['train'] = pipeline_train.result_storage['object_id_encoding']\n",
    "    load_pickle(task_df['test'], 'tmp/task_df_test')\n",
    "    load_pickle(task_df['train'], 'tmp/task_df_train')\n",
    "else:\n",
    "    task_df['test'] = from_pickle('tmp/task_df_test.pkl')\n",
    "    task_df['train'] = from_pickle('tmp/task_df_train.pkl')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Import necessairy methods for extraction"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "source": [
    "# transformers.py\n",
    "diff = tr.diff\n",
    "make_transformer = tr.make_transformer\n",
    "\n",
    "# aggregators.py\n",
    "take_subseries = agg.take_subseries\n",
    "reorder_columns = agg.reorder_columns\n",
    "aggregate_window_serieses = agg.aggregate_window_serieses\n",
    "create_aggregation_pipeline = agg.create_aggregation_pipeline"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Configuration"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "source": [
    "AGGREGATION_CFG = {\n",
    "    'target': {\n",
    "        'from': ['id_sales'],\n",
    "        'func_name': take_subseries,\n",
    "        'params': {\n",
    "            'columns': slice(-2, -1),\n",
    "            'new_name': ['target'],\n",
    "        },\n",
    "    },\n",
    "    'lags': {\n",
    "        'from': ['lags'],\n",
    "        'func_name': take_subseries,\n",
    "        'params': {\n",
    "            'columns': [f'lag_{i}' for i in [1, 2, 3, 4, 6, 12]],\n",
    "            'new_name': [f'lag_{i}' for i in [1, 2, 3, 4, 6, 12]],\n",
    "        },\n",
    "    },\n",
    "    'acf_lags': {\n",
    "        'from': ['lags_acf'],\n",
    "        'func_name': take_subseries,\n",
    "        'params': {\n",
    "            'columns': [f'acf_lag_{i}' for i in [1, 2, 3, 4, 6, 12]],\n",
    "            'new_name': [f'acf_lag_{i}' for i in [1, 2, 3, 4, 6, 12]],\n",
    "        },\n",
    "    },\n",
    "    'dynamic_aggregation': {\n",
    "        'from': ['train_series'],\n",
    "        'func_name': aggregate_window_serieses,\n",
    "        'params': {\n",
    "            'funcs': [\n",
    "                'mean', \n",
    "                'std',\n",
    "                'min',\n",
    "                'max',\n",
    "                make_transformer(np.percentile, q=20),\n",
    "                make_transformer(np.percentile, q=80),\n",
    "                ],\n",
    "            'windows': [2, 4, 6, 12],\n",
    "            'func_names': [\n",
    "                'mean',\n",
    "                'std',\n",
    "                'min',\n",
    "                'max',\n",
    "                'percentile_20',\n",
    "                'percentile_80',\n",
    "                ]\n",
    "        },\n",
    "    },\n",
    "    'diff_1_aggregation': {\n",
    "        'from': ['diff_1'],\n",
    "        'func_name': aggregate_window_serieses,\n",
    "        'params': {\n",
    "            'funcs': [\n",
    "                'mean', \n",
    "                'std',\n",
    "                'min',\n",
    "                'max',\n",
    "                ],\n",
    "            'windows': [2, 4, 6, 12],\n",
    "            'func_names': [\n",
    "                'mean',\n",
    "                'std',\n",
    "                'min',\n",
    "                'max',\n",
    "                ]\n",
    "        },\n",
    "    },\n",
    "    'diff_2_aggregation': {\n",
    "        'from': ['diff_2'],\n",
    "        'func_name': aggregate_window_serieses,\n",
    "        'params': {\n",
    "            'funcs': [\n",
    "                'mean', \n",
    "                'std',\n",
    "                'min',\n",
    "                'max',\n",
    "                ],\n",
    "            'windows': [2, 4, 6, 12],\n",
    "            'func_names': [\n",
    "                'mean',\n",
    "                'std',\n",
    "                'min',\n",
    "                'max',\n",
    "                ]\n",
    "        },\n",
    "    },\n",
    "}"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "source": [
    "GENERATOIN_CFG = {\n",
    "    'id_sales': {\n",
    "        'series_order': [],\n",
    "        'func_name': from_pickle,\n",
    "        'params': {\n",
    "            'path': 'tmp/id_sales.pkl'\n",
    "        }\n",
    "    },\n",
    "    'train_series': {\n",
    "        'series_order': ['id_sales'],\n",
    "        'func_name': take_subseries,\n",
    "        'params': { \n",
    "            'columns': slice(None,-2), \n",
    "            'new_name': [i for i in range(32)]\n",
    "        }\n",
    "    },\n",
    "    'diff_1': {\n",
    "        'series_order': ['train_series'], \n",
    "        'func_name': diff,\n",
    "        'params': {\n",
    "            'order': 1\n",
    "        }\n",
    "    },\n",
    "    'diff_2': {\n",
    "        'series_order': ['train_series'], \n",
    "        'func_name': diff,\n",
    "        'params': {\n",
    "            'order': 2\n",
    "        }\n",
    "    },\n",
    "    'acf': {\n",
    "        'series_order': [],\n",
    "        'func_name': from_pickle,\n",
    "        'params': {\n",
    "            'path': 'tmp/acf.pkl'\n",
    "        }\n",
    "    },\n",
    "    'lags': {\n",
    "        'series_order': ['train_series'],\n",
    "        'func_name': create_aggregation_pipeline,\n",
    "        'params': {\n",
    "            'func_queue': [\n",
    "                make_transformer(\n",
    "                    take_subseries, \n",
    "                    columns=slice(-13, -1), \n",
    "                    new_name=[f'lag_{13 - i}' for i in range(1, 13)]),\n",
    "                make_transformer(\n",
    "                    reorder_columns,\n",
    "                    new_order=[f'lag_{i}' for i in range(1, 13)])\n",
    "            ]\n",
    "        }\n",
    "    },\n",
    "    'lags_12': {\n",
    "        'series_order': ['train_series'],\n",
    "        'func_name': create_aggregation_pipeline,\n",
    "        'params': {\n",
    "            'func_queue': [\n",
    "                make_transformer(\n",
    "                    take_subseries, \n",
    "                    columns=slice(-25, -13), \n",
    "                    new_name=[f'lag_12_{13 - i}' for i in range(1, 13)]),\n",
    "                make_transformer(\n",
    "                    reorder_columns,\n",
    "                    new_order=[f'lag_12_{i}' for i in range(1, 13)])\n",
    "            ]\n",
    "        }\n",
    "    },\n",
    "    'lags_acf_raw': {\n",
    "        'series_order': ['lags', 'acf'],\n",
    "        'func_name': np.multiply,\n",
    "        'params': {}\n",
    "    },\n",
    "    'lags_acf': {\n",
    "        'series_order': ['lags_acf_raw'],\n",
    "        'func_name': take_subseries,\n",
    "        'params': {\n",
    "            'columns':slice(None, None, None), \n",
    "            'new_name':[f'acf_lag_{i}' for i in range(1, 13)],\n",
    "        }\n",
    "    },\n",
    "}"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "source": [
    "baseline_extractor = FeatureExtractor(GENERATOIN_CFG, AGGREGATION_CFG, ['id'])\n",
    "\n",
    "baseline_train = baseline_extractor({\n",
    "    'original': task_df['train']\n",
    "})"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/home/denissimo/Repo/fs_project/modules/transformers.py:16: FutureWarning: Calling a ufunc on non-aligned DataFrames (or DataFrame/Series combination). Currently, the indices are ignored and the result takes the index/columns of the first DataFrame. In the future , the DataFrames/Series will be aligned before applying the ufunc.\n",
      "Convert one of the arguments to a NumPy array (eg 'ufunc(df1, np.asarray(df2)') to keep the current behaviour, or align manually (eg 'df1, df2 = df1.align(df2)') before passing to the ufunc to obtain the future behaviour and silence this warning.\n",
      "  return func(*frames, **kwargs)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "source": [
    "with open('tmp/train_extracted.pkl', 'wb+') as writer:\n",
    "    pickle.dump(baseline_train['features'], writer)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "source": [
    "serieses = task_df['train'].groupby(['month_block', 'id']).item_cnt_day.sum().reset_index().set_index('id').index.value_counts()"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.10",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.10 64-bit ('env': venv)"
  },
  "interpreter": {
   "hash": "d6f0a83fd70573169f91d6afa8cd6daf12135fd479a43538d29ef02678dc9121"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}